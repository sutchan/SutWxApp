#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶å: restore-and-fix.py
ç‰ˆæœ¬å·: 1.0.0
æ›´æ–°æ—¥æœŸ: 2025-11-27
åŠŸèƒ½æè¿°: æ¢å¤æ–‡ä»¶å¤‡ä»½å¹¶ä½¿ç”¨ä¸“ä¸šæ–¹æ³•ä¿®å¤ç¼–ç 
"""

import shutil
from pathlib import Path
import chardet

def restore_and_fix():
    """
    æ¢å¤æ–‡ä»¶å¤‡ä»½å¹¶ä¿®å¤ç¼–ç 
    """
    # å®šä¹‰æ–‡ä»¶è·¯å¾„
    file2_path = Path(r"e:\Dropbox\GitHub\SutWxApp\docs\11-ä¼˜åŒ–å»ºè®®\02-ç³»ç»Ÿä¼˜åŒ–å»ºè®®.md")
    
    # æŸ¥æ‰¾å¤‡ä»½æ–‡ä»¶
    backup_files = list(Path(r"e:\Dropbox\GitHub\SutWxApp\docs\11-ä¼˜åŒ–å»ºè®®").glob("02-ç³»ç»Ÿä¼˜åŒ–å»ºè®®.md.bak*"))
    if not backup_files:
        print("âŒ æœªæ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")
        return False
    
    # ä½¿ç”¨æœ€æ–°çš„å¤‡ä»½æ–‡ä»¶
    backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    latest_backup = backup_files[0]
    print(f"âœ… æ‰¾åˆ°æœ€æ–°å¤‡ä»½: {latest_backup}")
    
    # æ¢å¤å¤‡ä»½
    shutil.copy2(latest_backup, file2_path)
    print(f"âœ… å·²æ¢å¤æ–‡ä»¶: {file2_path}")
    
    # ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
    with open(file2_path, 'rb') as f:
        raw_data = f.read()
    
    # ç§»é™¤UTF-8 BOMï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    bom_removed = False
    if raw_data.startswith(b'\xef\xbb\xbf'):
        raw_data = raw_data[3:]
        bom_removed = True
    
    # æ£€æµ‹ç¼–ç 
    result = chardet.detect(raw_data)
    detected_encoding = result['encoding']
    confidence = result['confidence']
    print(f"ğŸ” æ£€æµ‹åˆ°ç¼–ç : {detected_encoding}, ç½®ä¿¡åº¦: {confidence:.2f}")
    
    # å°è¯•ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è§£ç 
    try:
        content = raw_data.decode(detected_encoding)
        print(f"âœ… ä½¿ç”¨ {detected_encoding} è§£ç æˆåŠŸ")
        
        # éªŒè¯ä¸­æ–‡æ¯”ä¾‹
        chinese_chars = sum(1 for c in content if '\u4e00' <= c <= '\u9fff')
        total_chars = len(content)
        chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
        print(f"ğŸ“Š ä¸­æ–‡æ¯”ä¾‹: {chinese_ratio:.2f}")
        
        # å†™å…¥ä¸ºUTF-8æ— BOMæ ¼å¼
        with open(file2_path, 'w', encoding='utf-8', newline='\n') as f:
            f.write(content)
        
        print(f"âœ… ä¿®å¤æˆåŠŸ: {file2_path}")
        return True
        
    except UnicodeDecodeError:
        print(f"âŒ ä½¿ç”¨ {detected_encoding} è§£ç å¤±è´¥")
        
        # å°è¯•å…¶ä»–å¸¸è§ç¼–ç 
        common_encodings = ['utf-8', 'gbk', 'gb2312', 'big5', 'utf-16-le', 'utf-16-be']
        for enc in common_encodings:
            try:
                content = raw_data.decode(enc)
                print(f"âœ… ä½¿ç”¨ {enc} è§£ç æˆåŠŸ")
                
                # éªŒè¯ä¸­æ–‡æ¯”ä¾‹
                chinese_chars = sum(1 for c in content if '\u4e00' <= c <= '\u9fff')
                total_chars = len(content)
                chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
                print(f"ğŸ“Š ä¸­æ–‡æ¯”ä¾‹: {chinese_ratio:.2f}")
                
                if chinese_ratio > 0.5:
                    # å†™å…¥ä¸ºUTF-8æ— BOMæ ¼å¼
                    with open(file2_path, 'w', encoding='utf-8', newline='\n') as f:
                        f.write(content)
                    
                    print(f"âœ… ä¿®å¤æˆåŠŸ: {file2_path}")
                    return True
                    
            except UnicodeDecodeError:
                print(f"âŒ ä½¿ç”¨ {enc} è§£ç å¤±è´¥")
                continue
    
    print(f"âŒ æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥")
    return False

def main():
    """
    ä¸»å‡½æ•°
    """
    print("å¼€å§‹æ¢å¤å¹¶ä¿®å¤æ–‡ä»¶...")
    print("=" * 50)
    
    if restore_and_fix():
        print("âœ… ä¿®å¤å®Œæˆ")
    else:
        print("âŒ ä¿®å¤å¤±è´¥")
    
    print("=" * 50)

if __name__ == "__main__":
    main()
